{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8037091f-3315-4860-9882-20b02b88e333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pyspark\n",
      "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting py4j==0.10.9.7\n",
      "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.5/200.5 KB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488513 sha256=8af26040a1dcff53e73b6a20e2c005efb5a2a0d638585cb670010aaf1b8d1fd8\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.7 pyspark-3.5.1\n"
     ]
    }
   ],
   "source": [
    "!pip3 install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a80099ee-ee6f-4119-81c4-ab6f93b049ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting findspark\n",
      "  Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
      "Installing collected packages: findspark\n",
      "Successfully installed findspark-2.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip3 install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bf7a31c8-e2e4-4c22-9758-c31611ee808f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ubuntu/spark-3.3.1-bin-hadoop3'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init('/home/ubuntu/spark-3.3.1-bin-hadoop3')\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2bb42339-528b-48e2-a315-19f7d5a53890",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# The entry point into all functionality in Spark is the SparkSession class.\n",
    "spark = (SparkSession\n",
    "\t.builder\n",
    "\t.appName(\"DS5110/CS5501: my awesome Spark program\")\n",
    "\t.master(\"spark://172.31.24.10:7077\")\n",
    "\t.config(\"spark.executor.memory\", \"1024M\")\n",
    "\t.getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3812cb5b-386a-4fdc-8cf7-8e90919e0b9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4347626-43a4-485d-8298-fd44ee2315c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "319c8c38-285c-467d-94ba-436b05de3246",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5fa67c4-d936-4ef9-a45e-227913f48da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import asc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c5bd40f-0769-439a-ba91-4e8f1bf6ac75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# You can read the data from a file into DataFrames\n",
    "df = spark.read.csv(\"hdfs://172.31.24.10:9000/export.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76cf2187-969e-4e33-ad01-431e30027493",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc062dd0-9c96-4065-aa1c-fa25abec5520",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(_c0='battery_level', _c1='c02_level', _c2='cca2', _c3='cca3', _c4='cn', _c5='device_id', _c6='device_name', _c7='humidity', _c8='ip', _c9='latitude', _c10='lcd', _c11='longitude', _c12='scale', _c13='temp', _c14='timestamp')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "396d16af-92c1-4231-89f1-2f35b34ae8c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_c0: string, _c1: string, _c2: string, _c3: string, _c4: string, _c5: string, _c6: string, _c7: string, _c8: string, _c9: string, _c10: string, _c11: string, _c12: string, _c13: string, _c14: string]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "707505b8-1380-4310-b831-fee83d2f6a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[battery_level: int, c02_level: int, cca2: string, cca3: string, cn: string, device_id: int, device_name: string, humidity: int, ip: string, latitude: double, lcd: string, longitude: double, scale: string, temp: int, timestamp: bigint]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = (spark.read\n",
    "          .format(\"csv\")\n",
    "          .option(\"inferSchema\", True)\n",
    "          .option(\"header\", True)\n",
    "          .load(\"hdfs://172.31.24.10:9000/export.csv\"))\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf0774c4-542d-4935-966d-e11d42b91e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Row(battery_level=8, c02_level=868, cca2='US', cca3='USA', cn='United States', device_id=1, device_name='meter-gauge-1xbYRYcj', humidity=51, ip='68.161.225.1', latitude=38.0, lcd='green', longitude=-97.0, scale='Celsius', temp=34, timestamp=1458444054093)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6fb320cd-3814-44c0-94e5-3540714b251e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+----+----+-------------+---------+--------------------+--------+---------------+--------+------+---------+-------+----+-------------+\n",
      "|battery_level|c02_level|cca2|cca3|           cn|device_id|         device_name|humidity|             ip|latitude|   lcd|longitude|  scale|temp|    timestamp|\n",
      "+-------------+---------+----+----+-------------+---------+--------------------+--------+---------------+--------+------+---------+-------+----+-------------+\n",
      "|            8|      868|  US| USA|United States|        1|meter-gauge-1xbYRYcj|      51|   68.161.225.1|    38.0| green|    -97.0|Celsius|  34|1458444054093|\n",
      "|            7|     1473|  NO| NOR|       Norway|        2|   sensor-pad-2n2Pea|      70|  213.161.254.1|   62.47|   red|     6.15|Celsius|  11|1458444054119|\n",
      "|            2|     1556|  IT| ITA|        Italy|        3| device-mac-36TWSKiT|      44|      88.36.5.1|   42.83|   red|    12.83|Celsius|  19|1458444054120|\n",
      "|            6|     1080|  US| USA|United States|        4|   sensor-pad-4mzWkz|      32|  66.39.173.154|   44.06|yellow|  -121.32|Celsius|  28|1458444054121|\n",
      "|            4|      931|  PH| PHL|  Philippines|        5|therm-stick-5gimp...|      62|    203.82.41.9|   14.58| green|   120.97|Celsius|  25|1458444054122|\n",
      "|            3|     1210|  US| USA|United States|        6|sensor-pad-6al7RT...|      51| 204.116.105.67|   35.93|yellow|   -85.46|Celsius|  27|1458444054122|\n",
      "|            3|     1129|  CN| CHN|        China|        7|meter-gauge-7GeDoanM|      26|  220.173.179.1|   22.82|yellow|   108.32|Celsius|  18|1458444054123|\n",
      "|            0|     1536|  JP| JPN|        Japan|        8|sensor-pad-8xUD6p...|      35|  210.173.177.1|   35.69|   red|   139.69|Celsius|  27|1458444054123|\n",
      "|            3|      807|  JP| JPN|        Japan|        9| device-mac-9GcjZ2pw|      85|  118.23.68.227|   35.69| green|   139.69|Celsius|  13|1458444054124|\n",
      "|            7|     1470|  US| USA|United States|       10|sensor-pad-10Bsyw...|      56|208.109.163.218|   33.61|   red|  -111.89|Celsius|  26|1458444054125|\n",
      "+-------------+---------+----+----+-------------+---------+--------------------+--------+---------------+--------+------+---------+-------+----+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40569254-b10d-4484-a651-b6c5efa4a805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- battery_level: integer (nullable = true)\n",
      " |-- c02_level: integer (nullable = true)\n",
      " |-- cca2: string (nullable = true)\n",
      " |-- cca3: string (nullable = true)\n",
      " |-- cn: string (nullable = true)\n",
      " |-- device_id: integer (nullable = true)\n",
      " |-- device_name: string (nullable = true)\n",
      " |-- humidity: integer (nullable = true)\n",
      " |-- ip: string (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- lcd: string (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- scale: string (nullable = true)\n",
      " |-- temp: integer (nullable = true)\n",
      " |-- timestamp: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2427b47e-c89c-423d-a88d-14481f2e9b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+----+----+--------------------+---------+--------------------+--------+---------------+--------+------+---------+-------+----+-------------+\n",
      "|battery_level|c02_level|cca2|cca3|                  cn|device_id|         device_name|humidity|             ip|latitude|   lcd|longitude|  scale|temp|    timestamp|\n",
      "+-------------+---------+----+----+--------------------+---------+--------------------+--------+---------------+--------+------+---------+-------+----+-------------+\n",
      "|            5|     1217|  AE| ARE|United Arab Emirates|      501|device-mac-501e4O...|      48|  213.42.16.154|    24.0|yellow|     54.0|Celsius|  16|1458444054343|\n",
      "|            0|      915|  AR| ARG|           Argentina|      227|meter-gauge-2273p...|      34|  200.71.230.81|   -34.6| green|   -58.38|Celsius|  15|1458444054251|\n",
      "|            1|     1189|  AR| ARG|           Argentina|      319|meter-gauge-319Y3...|      54| 200.71.236.145|   -34.6|yellow|   -58.38|Celsius|  25|1458444054287|\n",
      "|            8|     1386|  AR| ARG|           Argentina|      763|meter-gauge-763JW...|      82|    200.55.0.70|   -34.6|yellow|   -58.38|Celsius|  21|1458444054404|\n",
      "|            0|      861|  AR| ARG|           Argentina|      943|meter-gauge-943BT...|      77|  200.59.128.19|   -34.6| green|   -58.38|Celsius|  33|1458444054435|\n",
      "|            5|      939|  AT| AUT|             Austria|       21|  device-mac-21sjz5h|      44|193.200.142.254|    48.2| green|    16.37|Celsius|  30|1458444054131|\n",
      "|            6|     1328|  AT| AUT|             Austria|       75|device-mac-75OLmC...|      96| 143.161.246.65|    48.2|yellow|    16.37|Celsius|  12|1458444054168|\n",
      "|            8|     1287|  AT| AUT|             Austria|      236|sensor-pad-2369xz...|      47|  217.25.119.17|    48.2|yellow|    16.37|Celsius|  22|1458444054256|\n",
      "|            2|     1522|  AT| AUT|             Austria|      257|meter-gauge-257AT...|      26|   87.243.133.1|    47.2|   red|    14.83|Celsius|  16|1458444054266|\n",
      "|            1|      811|  AT| AUT|             Austria|      271|meter-gauge-271BjIL0|      31|  149.148.140.1|    48.2| green|    16.37|Celsius|  16|1458444054271|\n",
      "|            7|      904|  AT| AUT|             Austria|      294|sensor-pad-294FMZ...|      26|     83.65.45.1|    48.2| green|    16.37|Celsius|  14|1458444054279|\n",
      "|            6|      917|  AT| AUT|             Austria|      369|device-mac-369rYH...|      25|  193.239.188.1|    48.2| green|    16.37|Celsius|  23|1458444054303|\n",
      "|            3|      826|  AT| AUT|             Austria|      483|device-mac-483Tyi...|      90| 84.116.245.201|    48.2| green|    16.37|Celsius|  16|1458444054339|\n",
      "|            2|      816|  AT| AUT|             Austria|      504|sensor-pad-504Kdi...|      78| 87.243.151.193|   47.27| green|     11.4|Celsius|  32|1458444054344|\n",
      "|            1|     1196|  AT| AUT|             Austria|      585|device-mac-5851AntHC|      51|   84.116.252.9|    48.2|yellow|    16.37|Celsius|  27|1458444054364|\n",
      "|            4|     1042|  AT| AUT|             Austria|      758| sensor-pad-7589QBtr|      48|   62.218.4.130|    48.2|yellow|    16.37|Celsius|  30|1458444054403|\n",
      "|            5|     1543|  AT| AUT|             Austria|      767|meter-gauge-767rd...|      65|  195.222.121.1|    48.2|   red|    16.37|Celsius|  19|1458444054405|\n",
      "|            0|      941|  AT| AUT|             Austria|      974|sensor-pad-974x9dkX1|      53| 84.116.216.166|    48.2| green|    16.37|Celsius|  26|1458444054439|\n",
      "|            8|      895|  AT| AUT|             Austria|      977|meter-gauge-977yB...|      52|     83.65.95.1|    48.2| green|    16.37|Celsius|  11|1458444054440|\n",
      "|            0|      899|  AU| AUS|           Australia|      111|device-mac-111WYt...|      32| 203.123.94.193|   -27.0| green|    133.0|Celsius|  16|1458444054189|\n",
      "+-------------+---------+----+----+--------------------+---------+--------------------+--------+---------------+--------+------+---------+-------+----+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Here is the main part of the part 2 \"\"\"\n",
    "# sort the DataFrame by country code and then timestamp\n",
    "sorted_df = df.orderBy(asc(\"cca2\"), asc(\"timestamp\"))\n",
    "\n",
    "# show the sorted DataFrame\n",
    "sorted_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0390bb8-6c5c-4c1a-be0b-6312db7789a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# write the sorted DataFrame back to HDFS\n",
    "sorted_df.write.format(\"csv\").mode(\"overwrite\").save(\"hdfs://172.31.24.10:9000/part2_output.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c171e8c-7037-4ad5-926f-cb4238565f4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9f036c7f-6301-4b35-82b0-b69f7af44e5f",
   "metadata": {},
   "source": [
    "# Part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4396d52-14c8-4d04-a8ef-1a6d9d81e7d5",
   "metadata": {},
   "source": [
    "## Part3 - Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1c285562-152a-40ad-9eb4-3e1a7db290f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is a skeleton of the PageRank algorithm.\n",
    "Feel free to use any piece of code in this provided skeleton source file.\n",
    "To use it, you will need to copy it into your Notebook. \n",
    "Feel free to make modifications to template code as you see fit.\n",
    "However, you are encouraged to implement the algorithm completely on\n",
    "your own. :-)\n",
    "\"\"\"\n",
    "import re\n",
    "import sys\n",
    "from operator import add\n",
    "from typing import Iterable, Tuple\n",
    "\n",
    "from pyspark.resultiterable import ResultIterable\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "50401fa9-2fd7-4e66-b50d-6f68f13f45e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Helper function to calculates URL contributions to the rank of other URLs\"\"\"\n",
    "def calculateRankContrib(urls: ResultIterable[str], rank: float) -> Iterable[Tuple[str, float]]:\n",
    "    num_urls = len(urls)\n",
    "    for url in urls:\n",
    "        yield (url, rank / num_urls)\n",
    "\n",
    "\n",
    "\"\"\"Helper function to parses a urls string into urls pair\"\"\"\n",
    "def parseNeighborURLs(urls: str) -> Tuple[str, str]:\n",
    "    parts = re.split(r'\\s+', urls)\n",
    "    return parts[0], parts[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "47840518-e465-434e-87e3-eb3fa7137098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/02/28 03:40:56 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Spark context\n",
    "# TODO: You should define a new name for your PySpark PageRank program\n",
    "spark = (SparkSession.builder.appName(\"A2:PageRank-Task1\")\n",
    "    .master(\"spark://172.31.24.10:7077\")\n",
    "    .config(\"spark.executor.memory\", \"2048M\")\n",
    "    .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d82dc9f6-bb11-4452-aac3-fd6fdc5477ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Hint: You want to use hash partitioning to optimize the performance of join\"\"\"\n",
    "\"\"\" For task 1, as required, we do not use Partitioning \"\"\"\n",
    "\n",
    "# Loads in input file\n",
    "#     URL         neighbor URL\n",
    "#     URL         neighbor URL\n",
    "#     URL         neighbor URL\n",
    "#     ...\n",
    "linesRDD_no_filter = spark.sparkContext.textFile(\"hdfs://172.31.24.10:9000/web-BerkStan.txt\")\n",
    "# exclude the the empty rows starts with #\n",
    "linesRDD = linesRDD_no_filter.filter(lambda line : not line.startswith('#'))\n",
    "\n",
    "# Perform a transformation to define a links RDD by using parseNeighborURLs helper function\n",
    "linksRDD = linesRDD.map(lambda urls: parseNeighborURLs(urls)).distinct().groupByKey()\n",
    "\n",
    "# Initialize a ranks RDD\n",
    "# as required, we add persist() to persist the ranks RDD in memory or disk\n",
    "ranksRDD = linksRDD.map(lambda url_neighbors: (url_neighbors[0], 1.0)).persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066a2d02-5a5a-4c51-ae49-7b05e2cd3273",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb4f3c9-0485-4e42-8619-bb27748468ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2505fe3e-c8e1-43ca-80da-b474b00365aa",
   "metadata": {},
   "source": [
    "### ----------------------------------------------------------------------------------------------------------------------------------------------- Following section is for testing and getting familiar with the data and spark, if you are grading my assignment, please just ignore this section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e89e1758-656f-4c5f-b811-0264d456ff94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[92] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linksRDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1ca41c0f-20f3-4299-b012-98607e73cb0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[93] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranksRDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "59827f06-9272-4f16-8d89-e9a3231fc5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_join = linksRDD.join(ranksRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "027b7846-687b-46b0-84c7-909e26c43ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sample_data = table_join.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6d766b27-d71d-48ca-b7d6-3e10ac6537ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('722', (<pyspark.resultiterable.ResultIterable object at 0x7fa1c7a08520>, 1.0))\n",
      "('77', (<pyspark.resultiterable.ResultIterable object at 0x7fa1c7a083a0>, 1.0))\n",
      "('143', (<pyspark.resultiterable.ResultIterable object at 0x7fa1c7a08700>, 1.0))\n",
      "('256', (<pyspark.resultiterable.ResultIterable object at 0x7fa1c7a080d0>, 1.0))\n",
      "('330', (<pyspark.resultiterable.ResultIterable object at 0x7fa1c7a08250>, 1.0))\n",
      "('483', (<pyspark.resultiterable.ResultIterable object at 0x7fa1c7a08100>, 1.0))\n",
      "('61273', (<pyspark.resultiterable.ResultIterable object at 0x7fa1c7a082b0>, 1.0))\n",
      "('801', (<pyspark.resultiterable.ResultIterable object at 0x7fa1c7a08220>, 1.0))\n",
      "('158745', (<pyspark.resultiterable.ResultIterable object at 0x7fa1c7a081f0>, 1.0))\n",
      "('172984', (<pyspark.resultiterable.ResultIterable object at 0x7fa1c7a08df0>, 1.0))\n"
     ]
    }
   ],
   "source": [
    "for data in sample_data:\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4ad8be0b-a7e0-4461-9dc2-bd16914b1673",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linesRDD.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "80ebaabc-6337-45b0-9f75-e1cd9f8a46b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linksRDD.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f7c9d02e-7b84-4c61-a35d-572cbcc5b5f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranksRDD.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4d8897-287d-4f57-bd9c-417b08df1fbd",
   "metadata": {},
   "source": [
    "### ----------------------------------------------------------------------------------------------------------------------------------------------- my testing ends here, following are my assignment code for grading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7488f3e1-4419-4aac-ace6-3e753679d6c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da4e612-5446-4344-a26e-5a44995c86f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a02e88-eb23-40a2-9008-adf8be0e2047",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c4c5ba-3374-4094-bc5f-40031d34b9f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7e712f0a-d38b-4725-9843-a0033ec58a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates and updates URL ranks continuously using PageRank algorithm.\n",
    "# Replace N with a number ranging from 3 to 10\n",
    "# For A2 you are required to complete 10 iterations\n",
    "N = 10\n",
    "for iteration in range(N):\n",
    "    # TODO: Implement the PageRank algorithm here\n",
    "    # TODO: Implement the PageRank algorithm here\n",
    "    # TODO: Implement the PageRank algorithm here\n",
    "    \n",
    "    # calculate URL contributions to the rank of other URLs\n",
    "    contributionsRDD = linksRDD.join(ranksRDD).flatMap(lambda x: calculateRankContrib(x[1][0], x[1][1]))\n",
    "\n",
    "    # Update URL ranks using the PageRank algorithm\n",
    "    # as required, we add persist() to persist the ranks RDD in memory or disk\n",
    "    ranksRDD = contributionsRDD.reduceByKey(lambda x, y: x + y).mapValues(lambda rank: 0.15 + 0.85 * rank).persist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ab4db6-ab99-45b9-857d-cce31bceb383",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2600c422-f486-4af9-a56a-29e4dca7b360",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4f5e0b7f-b462-40a3-9d54-aa36cdc18f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 22:==================================================>     (20 + 2) / 22]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first 50 rows with the highest ranks:\n",
      "272919: 6531.324623752456\n",
      "438238: 4335.323158564437\n",
      "571448: 2383.89760741189\n",
      "601656: 2195.3940755967287\n",
      "316792: 1855.6908757901515\n",
      "319209: 1632.8193684975693\n",
      "184094: 1532.284237448339\n",
      "571447: 1492.930163093879\n",
      "401873: 1436.1600933469288\n",
      "66244: 1261.578395867334\n",
      "68949: 1260.7919421349131\n",
      "284306: 1257.2475650644853\n",
      "68948: 1251.1723536459224\n",
      "96070: 1235.2985405976256\n",
      "95552: 1235.2985405976253\n",
      "86238: 1235.2985405976253\n",
      "77284: 1235.2985405976253\n",
      "68946: 1235.2985405976253\n",
      "86239: 1235.2985405976253\n",
      "66909: 1235.2985405976253\n",
      "86237: 1235.2985405976253\n",
      "95551: 1235.2985405976253\n",
      "68947: 1235.2985405976253\n",
      "768: 1225.5975665113076\n",
      "927: 1117.8383051141836\n",
      "210376: 920.6701252803681\n",
      "95527: 919.6797146521095\n",
      "100130: 916.0190658202691\n",
      "101163: 912.5380530105953\n",
      "95018: 911.1831080077993\n",
      "100646: 909.7095673033018\n",
      "96045: 904.3981315809754\n",
      "66879: 895.7909746044769\n",
      "210305: 893.0386730972408\n",
      "319412: 887.9352083382672\n",
      "571451: 875.7852546255615\n",
      "570985: 871.5825582573231\n",
      "544858: 869.6096568148237\n",
      "184142: 863.230778184179\n",
      "299039: 832.3149809807293\n",
      "49176: 819.8687801616519\n",
      "299040: 784.9195782082288\n",
      "319210: 764.4429282969843\n",
      "184332: 748.1100966771177\n",
      "184279: 743.4092370378008\n",
      "743: 694.557357008995\n",
      "313077: 681.9298001499117\n",
      "331840: 665.4905257656992\n",
      "33: 660.9927237591638\n",
      "184150: 649.4401909507077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Show the first 50 rows with the highest ranks\n",
    "top_50_ranks = ranksRDD.takeOrdered(50, key=lambda x: -x[1])\n",
    "print(\"The first 50 rows with the highest ranks:\")\n",
    "for url, rank in top_50_ranks:\n",
    "    print(\"{}: {}\".format(url, rank))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f973bb-f128-4fb1-b7e7-d69b7d915359",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a42f4ce-7655-45af-9238-1f9815270ef6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "663f5bf3-539d-43e9-9356-9b34a19c6e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Save output to HDFS\n",
    "ranksDf = ranksRDD.toDF()\n",
    "# Note: You should not overwrite the input file. Otherwise you need to reload it\n",
    "ranksDf.write.format(\"csv\").mode(\"overwrite\").save(\"hdfs://172.31.24.10:9000/part3_task1_output.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "95ec1143-13ca-45a1-ad01-c92c74629b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optionally) stop the Spark session when it's done\n",
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffe4f93-1924-4d21-b3e4-c44edb9d0818",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "606a1b5c-22c7-4468-a5f8-a7fbf1da319e",
   "metadata": {},
   "source": [
    "## Part 3 - Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9c8815-9752-4784-aa2c-ef38a2b9f00f",
   "metadata": {},
   "source": [
    "### task2-1 Try to persist every iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9d8dfb3f-bdb9-4838-b180-7e1d15056acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/02/28 04:25:56 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Spark context\n",
    "# TODO: You should define a new name for your PySpark PageRank program\n",
    "spark = (SparkSession.builder.appName(\"A2:PageRank-Task2\")\n",
    "    .master(\"spark://172.31.24.10:7077\")\n",
    "    .config(\"spark.executor.memory\", \"2048M\")\n",
    "    .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "712b323b-c8c9-4cea-aaaa-f6b593f0f958",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Hint: You want to use hash partitioning to optimize the performance of join\"\"\"\n",
    "\"\"\" For task 2, as required, we will use Partitioning, by using partitionBy() method \"\"\"\n",
    "\n",
    "# Loads in input file\n",
    "#     URL         neighbor URL\n",
    "#     URL         neighbor URL\n",
    "#     URL         neighbor URL\n",
    "#     ...\n",
    "linesRDD_no_filter = spark.sparkContext.textFile(\"hdfs://172.31.24.10:9000/web-BerkStan.txt\")\n",
    "linesRDD = linesRDD_no_filter.filter(lambda line : not line.startswith('#'))\n",
    "\n",
    "# the number of partitions, task 1 is 2, so in task 2, we will try 4\n",
    "partition_N = 4\n",
    "\n",
    "# Perform a transformation to define a links RDD by using parseNeighborURLs helper function\n",
    "# we partition the links RDD into N parts\n",
    "linksRDD = linesRDD.map(lambda urls: parseNeighborURLs(urls)).distinct().groupByKey().partitionBy(partition_N)\n",
    "\n",
    "# Initialize a ranks RDD\n",
    "# we partition the ranks RDD into N parts\n",
    "# and as required, we add persist() to persist the ranks RDD in memory or disk\n",
    "ranksRDD = linksRDD.map(lambda url_neighbors: (url_neighbors[0], 1.0)).partitionBy(partition_N).persist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2106e2cc-cfc1-4516-8e0d-b890a940eb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates and updates URL ranks continuously using PageRank algorithm.\n",
    "# Replace N with a number ranging from 3 to 10\n",
    "# For A2 you are required to complete 10 iterations\n",
    "N = 10\n",
    "for iteration in range(N):\n",
    "    # TODO: Implement the PageRank algorithm here\n",
    "    # TODO: Implement the PageRank algorithm here\n",
    "    # TODO: Implement the PageRank algorithm here\n",
    "    \n",
    "    # calculate URL contributions to the rank of other URLs\n",
    "    contributionsRDD = linksRDD.join(ranksRDD).flatMap(lambda x: calculateRankContrib(x[1][0], x[1][1]))\n",
    "\n",
    "    # Update URL ranks using the PageRank algorithm\n",
    "    # as required, we add persist() to persist the ranks RDD in memory or disk\n",
    "    ranksRDD = contributionsRDD.reduceByKey(lambda x, y: x + y).mapValues(lambda rank: 0.15 + 0.85 * rank).persist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "25577395-fe1e-42b5-90a2-3c0d358147ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 37:>                                                         (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first 50 rows with the highest ranks:\n",
      "272919: 6531.324623752462\n",
      "438238: 4335.323158564417\n",
      "571448: 2383.897607411887\n",
      "601656: 2195.3940755967305\n",
      "316792: 1855.6908757901547\n",
      "319209: 1632.8193684975668\n",
      "184094: 1532.2842374483457\n",
      "571447: 1492.9301630938799\n",
      "401873: 1436.1600933469324\n",
      "66244: 1261.57839586733\n",
      "68949: 1260.791942134909\n",
      "284306: 1257.247565064486\n",
      "68948: 1251.1723536459187\n",
      "86237: 1235.2985405976215\n",
      "77284: 1235.2985405976215\n",
      "95551: 1235.2985405976215\n",
      "66909: 1235.2985405976212\n",
      "86238: 1235.2985405976212\n",
      "96070: 1235.2985405976212\n",
      "86239: 1235.298540597621\n",
      "68946: 1235.298540597621\n",
      "68947: 1235.2985405976206\n",
      "95552: 1235.2985405976206\n",
      "768: 1225.5975665113092\n",
      "927: 1117.838305114185\n",
      "210376: 920.670125280368\n",
      "95527: 919.6797146521053\n",
      "100130: 916.019065820265\n",
      "101163: 912.538053010591\n",
      "95018: 911.1831080077952\n",
      "100646: 909.709567303298\n",
      "96045: 904.3981315809713\n",
      "66879: 895.7909746044725\n",
      "210305: 893.0386730972408\n",
      "319412: 887.935208338267\n",
      "571451: 875.7852546255614\n",
      "570985: 871.582558257324\n",
      "544858: 869.6096568148233\n",
      "184142: 863.2307781841805\n",
      "299039: 832.3149809807302\n",
      "49176: 819.8687801616533\n",
      "299040: 784.9195782082285\n",
      "319210: 764.4429282969845\n",
      "184332: 748.1100966771195\n",
      "184279: 743.4092370378024\n",
      "743: 694.557357008996\n",
      "313077: 681.929800149912\n",
      "331840: 665.4905257656998\n",
      "33: 660.9927237591635\n",
      "184150: 649.4401909507085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Show the first 50 rows with the highest ranks\n",
    "top_50_ranks = ranksRDD.takeOrdered(50, key=lambda x: -x[1])\n",
    "print(\"The first 50 rows with the highest ranks:\")\n",
    "for url, rank in top_50_ranks:\n",
    "    print(\"{}: {}\".format(url, rank))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f78edf-c816-458f-95fd-3be00d9c73a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c755973c-9eae-4359-b94e-7986c630a557",
   "metadata": {},
   "source": [
    "### task2-2 Try to persist every 2 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ba22860e-2fa5-406c-8e20-6381565c7260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/02/28 04:00:54 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Spark context\n",
    "# TODO: You should define a new name for your PySpark PageRank program\n",
    "spark = (SparkSession.builder.appName(\"A2:PageRank-Task2-2\")\n",
    "    .master(\"spark://172.31.24.10:7077\")\n",
    "    .config(\"spark.executor.memory\", \"2048M\")\n",
    "    .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d9366d4b-a84d-47ba-8c54-a0cf68b9ad90",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Hint: You want to use hash partitioning to optimize the performance of join\"\"\"\n",
    "\"\"\" For task 2, as required, we will use Partitioning, by using partitionBy() method \"\"\"\n",
    "\"\"\" This time, we try to not call persist() every iteration, rather every 2 iterations \"\"\"\n",
    "\n",
    "# Loads in input file\n",
    "#     URL         neighbor URL\n",
    "#     URL         neighbor URL\n",
    "#     URL         neighbor URL\n",
    "#     ...\n",
    "linesRDD_no_filter = spark.sparkContext.textFile(\"hdfs://172.31.24.10:9000/web-BerkStan.txt\")\n",
    "linesRDD = linesRDD_no_filter.filter(lambda line : not line.startswith('#'))\n",
    "\n",
    "# the number of partitions, task 1 is 2, so in task 2, we will try 4\n",
    "partition_N = 4\n",
    "\n",
    "# Perform a transformation to define a links RDD by using parseNeighborURLs helper function\n",
    "# we partition the links RDD into N parts\n",
    "linksRDD = linesRDD.map(lambda urls: parseNeighborURLs(urls)).distinct().groupByKey().partitionBy(partition_N)\n",
    "\n",
    "# Initialize a ranks RDD\n",
    "# we partition the ranks RDD into N parts\n",
    "# and as required, we add persist() to persist the ranks RDD in memory or disk\n",
    "ranksRDD = linksRDD.map(lambda url_neighbors: (url_neighbors[0], 1.0)).partitionBy(partition_N).persist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a7c5a339-f669-47a1-b1c2-fe8ff2aa8957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates and updates URL ranks continuously using PageRank algorithm.\n",
    "# Replace N with a number ranging from 3 to 10\n",
    "# For A2 you are required to complete 10 iterations\n",
    "N = 10\n",
    "\n",
    "for iteration in range(N):\n",
    "    # TODO: Implement the PageRank algorithm here\n",
    "    # TODO: Implement the PageRank algorithm here\n",
    "    # TODO: Implement the PageRank algorithm here\n",
    "    \n",
    "    # calculate URL contributions to the rank of other URLs\n",
    "    contributionsRDD = linksRDD.join(ranksRDD).flatMap(lambda x: calculateRankContrib(x[1][0], x[1][1]))\n",
    "\n",
    "    # Update URL ranks using the PageRank algorithm\n",
    "    # as required, we add persist() to persist the ranks RDD in memory or disk\n",
    "    \"\"\" persist every 2 iterations \"\"\"\n",
    "    if not iteration % 2:\n",
    "        ranksRDD = contributionsRDD.reduceByKey(lambda x, y: x + y).mapValues(lambda rank: 0.15 + 0.85 * rank).persist()\n",
    "    else:\n",
    "        ranksRDD = contributionsRDD.reduceByKey(lambda x, y: x + y).mapValues(lambda rank: 0.15 + 0.85 * rank)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9f2e819d-1edc-4aa4-b75a-0afbf283b976",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 52:=============================>                            (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first 50 rows with the highest ranks:\n",
      "272919: 6531.324623752462\n",
      "438238: 4335.323158564447\n",
      "571448: 2383.897607411887\n",
      "601656: 2195.3940755967305\n",
      "316792: 1855.6908757901526\n",
      "319209: 1632.8193684975718\n",
      "184094: 1532.2842374483391\n",
      "571447: 1492.9301630938783\n",
      "401873: 1436.1600933469294\n",
      "66244: 1261.5783958673344\n",
      "68949: 1260.7919421349134\n",
      "284306: 1257.2475650644838\n",
      "68948: 1251.1723536459226\n",
      "66909: 1235.2985405976262\n",
      "95551: 1235.298540597626\n",
      "96070: 1235.2985405976258\n",
      "68947: 1235.2985405976249\n",
      "95552: 1235.2985405976249\n",
      "86237: 1235.2985405976249\n",
      "86239: 1235.2985405976249\n",
      "68946: 1235.2985405976249\n",
      "86238: 1235.2985405976249\n",
      "77284: 1235.2985405976249\n",
      "768: 1225.5975665113071\n",
      "927: 1117.8383051141816\n",
      "210376: 920.6701252803678\n",
      "95527: 919.6797146521111\n",
      "100130: 916.0190658202707\n",
      "101163: 912.538053010597\n",
      "95018: 911.1831080078009\n",
      "100646: 909.7095673033035\n",
      "96045: 904.3981315809768\n",
      "66879: 895.7909746044784\n",
      "210305: 893.038673097241\n",
      "319412: 887.935208338267\n",
      "571451: 875.7852546255582\n",
      "570985: 871.5825582573211\n",
      "544858: 869.6096568148232\n",
      "184142: 863.2307781841781\n",
      "299039: 832.3149809807298\n",
      "49176: 819.8687801616529\n",
      "299040: 784.9195782082282\n",
      "319210: 764.4429282969842\n",
      "184332: 748.1100966771166\n",
      "184279: 743.4092370377992\n",
      "743: 694.5573570089946\n",
      "313077: 681.9298001499127\n",
      "331840: 665.4905257656986\n",
      "33: 660.9927237591621\n",
      "184150: 649.4401909507081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Show the first 50 rows with the highest ranks\n",
    "top_50_ranks = ranksRDD.takeOrdered(50, key=lambda x: -x[1])\n",
    "print(\"The first 50 rows with the highest ranks:\")\n",
    "for url, rank in top_50_ranks:\n",
    "    print(\"{}: {}\".format(url, rank))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8235ac7-81aa-4416-8e30-ae0d7ca8a609",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebb2cf8-a187-4437-a25d-607fcfeae319",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9aa7e319-9d57-4cae-b498-37ab8744c05a",
   "metadata": {},
   "source": [
    "## Part 3 - Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec847af-8019-4008-a057-d5a2b7f5aa4f",
   "metadata": {},
   "source": [
    "### kill at arounf 65% of the progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "faa70af3-9575-4337-8910-ea1e741422b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/02/28 04:26:15 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Spark context\n",
    "# TODO: You should define a new name for your PySpark PageRank program\n",
    "spark = (SparkSession.builder.appName(\"A2:PageRank-Task3\")\n",
    "    .master(\"spark://172.31.24.10:7077\")\n",
    "    .config(\"spark.executor.memory\", \"2048M\")\n",
    "    .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d2edd512-4046-4fd8-bdf6-5ed4735116bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Hint: You want to use hash partitioning to optimize the performance of join\"\"\"\n",
    "\"\"\" For task 3, we will use the scenario used in task2 with persist for every iteration \"\"\"\n",
    "\n",
    "# Loads in input file\n",
    "#     URL         neighbor URL\n",
    "#     URL         neighbor URL\n",
    "#     URL         neighbor URL\n",
    "#     ...\n",
    "linesRDD_no_filter = spark.sparkContext.textFile(\"hdfs://172.31.24.10:9000/web-BerkStan.txt\")\n",
    "linesRDD = linesRDD_no_filter.filter(lambda line : not line.startswith('#'))\n",
    "\n",
    "# the number of partitions, task 1 is 2, so in task 2, we will try 4\n",
    "partition_N = 4\n",
    "\n",
    "# Perform a transformation to define a links RDD by using parseNeighborURLs helper function\n",
    "# we partition the links RDD into N parts\n",
    "linksRDD = linesRDD.map(lambda urls: parseNeighborURLs(urls)).distinct().groupByKey().partitionBy(partition_N)\n",
    "\n",
    "# Initialize a ranks RDD\n",
    "# we partition the ranks RDD into N parts\n",
    "# and as required, we add persist() to persist the ranks RDD in memory or disk\n",
    "ranksRDD = linksRDD.map(lambda url_neighbors: (url_neighbors[0], 1.0)).partitionBy(partition_N).persist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f7eb431b-db75-42c6-99f7-ca5c076aebb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates and updates URL ranks continuously using PageRank algorithm.\n",
    "# Replace N with a number ranging from 3 to 10\n",
    "# For A2 you are required to complete 10 iterations\n",
    "N = 10\n",
    "for iteration in range(N):\n",
    "    # TODO: Implement the PageRank algorithm here\n",
    "    # TODO: Implement the PageRank algorithm here\n",
    "    # TODO: Implement the PageRank algorithm here\n",
    "    \n",
    "    # calculate URL contributions to the rank of other URLs\n",
    "    contributionsRDD = linksRDD.join(ranksRDD).flatMap(lambda x: calculateRankContrib(x[1][0], x[1][1]))\n",
    "\n",
    "    # Update URL ranks using the PageRank algorithm\n",
    "    # as required, we add persist() to persist the ranks RDD in memory or disk\n",
    "    ranksRDD = contributionsRDD.reduceByKey(lambda x, y: x + y).mapValues(lambda rank: 0.15 + 0.85 * rank).persist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "10f1cc4f-d72b-4643-836f-af2b9349a86e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 66:>                                                         (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/02/28 04:31:11 ERROR TaskSchedulerImpl: Lost executor 1 on 172.31.27.103: worker lost\n",
      "24/02/28 04:31:11 WARN TaskSetManager: Lost task 2.0 in stage 66.0 (TID 442) (172.31.27.103 executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: worker lost\n",
      "24/02/28 04:31:11 WARN TaskSetManager: Lost task 0.0 in stage 66.0 (TID 440) (172.31.27.103 executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: worker lost\n",
      "24/02/28 04:31:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_584_2 !\n",
      "24/02/28 04:31:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_381_2 !\n",
      "24/02/28 04:31:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_336_2 !\n",
      "24/02/28 04:31:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_381_0 !\n",
      "24/02/28 04:31:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_197_2 !\n",
      "24/02/28 04:31:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_552_2 !\n",
      "24/02/28 04:31:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_173_0 !\n",
      "24/02/28 04:31:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_173_2 !\n",
      "24/02/28 04:31:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_213_2 !\n",
      "24/02/28 04:31:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_221_0 !\n",
      "24/02/28 04:31:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_536_0 !\n",
      "24/02/28 04:31:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_197_0 !\n",
      "24/02/28 04:31:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_157_0 !\n",
      "24/02/28 04:31:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_181_2 !\n",
      "24/02/28 04:31:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_165_2 !\n",
      "24/02/28 04:31:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_396_2 !\n",
      "24/02/28 04:31:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_229_0 !\n",
      "24/02/28 04:31:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_520_2 !\n",
      "24/02/28 04:31:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_205_2 !\n",
      "24/02/28 04:31:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_213_0 !\n",
      "24/02/28 04:31:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_568_2 !\n",
      "24/02/28 04:31:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_366_2 !\n",
      "24/02/28 04:31:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_576_2 !\n",
      "24/02/28 04:31:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_165_0 !\n",
      "24/02/28 04:31:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_181_0 !\n",
      "24/02/28 04:31:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_205_0 !\n",
      "24/02/28 04:31:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_0 !\n",
      "24/02/28 04:31:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_149_2 !\n",
      "24/02/28 04:31:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_351_0 !\n",
      "24/02/28 04:31:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_544_2 !\n",
      "24/02/28 04:31:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_221_2 !\n",
      "24/02/28 04:31:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_328_0 !\n",
      "24/02/28 04:31:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_592_2 !\n",
      "24/02/28 04:31:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_520_0 !\n",
      "24/02/28 04:31:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_351_2 !\n",
      "24/02/28 04:31:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_336_0 !\n",
      "24/02/28 04:31:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_528_2 !\n",
      "24/02/28 04:31:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_536_2 !\n",
      "24/02/28 04:31:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_560_2 !\n",
      "24/02/28 04:31:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_528_0 !\n",
      "24/02/28 04:31:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_584_0 !\n",
      "24/02/28 04:31:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_189_0 !\n",
      "24/02/28 04:31:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_560_0 !\n",
      "24/02/28 04:31:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_396_0 !\n",
      "24/02/28 04:31:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_189_2 !\n",
      "24/02/28 04:31:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_592_0 !\n",
      "24/02/28 04:31:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_552_0 !\n",
      "24/02/28 04:31:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_568_0 !\n",
      "24/02/28 04:31:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_229_2 !\n",
      "24/02/28 04:31:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_366_0 !\n",
      "24/02/28 04:31:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_544_0 !\n",
      "24/02/28 04:31:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_576_0 !\n",
      "24/02/28 04:31:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_157_2 !\n",
      "24/02/28 04:31:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_328_2 !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 66:==============>                                           (1 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/02/28 04:31:15 WARN TaskSetManager: Lost task 0.1 in stage 66.0 (TID 444) (172.31.24.10 executor 0): FetchFailed(BlockManagerId(1, 172.31.27.103, 34495, None), shuffleId=61, mapIndex=0, mapId=398, reduceId=0, message=\n",
      "org.apache.spark.shuffle.FetchFailedException\n",
      "\tat org.apache.spark.errors.SparkCoreErrors$.fetchFailedError(SparkCoreErrors.scala:312)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:1166)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:904)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:85)\n",
      "\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:732)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:438)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:272)\n",
      "Caused by: org.apache.spark.ExecutorDeadException: The relative remote executor(Id: 1), which maintains the block data to fetch is dead.\n",
      "\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:136)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.transferAllOutstanding(RetryingBlockTransferor.java:154)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.start(RetryingBlockTransferor.java:133)\n",
      "\tat org.apache.spark.network.netty.NettyBlockTransferService.fetchBlocks(NettyBlockTransferService.scala:146)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.sendRequest(ShuffleBlockFetcherIterator.scala:363)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.send$1(ShuffleBlockFetcherIterator.scala:1136)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchUpToMaxBytes(ShuffleBlockFetcherIterator.scala:1128)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:702)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:192)\n",
      "\tat org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:89)\n",
      "\tat org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:106)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\t... 8 more\n",
      "\n",
      ")\n",
      "24/02/28 04:31:15 WARN TaskSetManager: Lost task 2.1 in stage 66.0 (TID 445) (172.31.24.10 executor 0): FetchFailed(BlockManagerId(1, 172.31.27.103, 34495, None), shuffleId=61, mapIndex=0, mapId=398, reduceId=2, message=\n",
      "org.apache.spark.shuffle.FetchFailedException\n",
      "\tat org.apache.spark.errors.SparkCoreErrors$.fetchFailedError(SparkCoreErrors.scala:312)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:1166)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:904)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:85)\n",
      "\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:732)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:438)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:272)\n",
      "Caused by: org.apache.spark.ExecutorDeadException: The relative remote executor(Id: 1), which maintains the block data to fetch is dead.\n",
      "\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:136)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.transferAllOutstanding(RetryingBlockTransferor.java:154)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.start(RetryingBlockTransferor.java:133)\n",
      "\tat org.apache.spark.network.netty.NettyBlockTransferService.fetchBlocks(NettyBlockTransferService.scala:146)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.sendRequest(ShuffleBlockFetcherIterator.scala:363)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.send$1(ShuffleBlockFetcherIterator.scala:1136)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchUpToMaxBytes(ShuffleBlockFetcherIterator.scala:1128)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:702)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:192)\n",
      "\tat org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:89)\n",
      "\tat org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:106)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\t... 8 more\n",
      "\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 67:=============================>                            (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first 50 rows with the highest ranks:\n",
      "272919: 6531.324623752464\n",
      "438238: 4335.323158564445\n",
      "571448: 2383.8976074118905\n",
      "601656: 2195.394075596731\n",
      "316792: 1855.6908757901533\n",
      "319209: 1632.8193684975713\n",
      "184094: 1532.2842374483419\n",
      "571447: 1492.9301630938785\n",
      "401873: 1436.1600933469288\n",
      "66244: 1261.5783958673337\n",
      "68949: 1260.7919421349134\n",
      "284306: 1257.2475650644833\n",
      "68948: 1251.172353645923\n",
      "96070: 1235.298540597627\n",
      "66909: 1235.2985405976258\n",
      "95552: 1235.2985405976253\n",
      "86237: 1235.2985405976253\n",
      "86239: 1235.2985405976253\n",
      "68946: 1235.2985405976253\n",
      "86238: 1235.2985405976253\n",
      "68947: 1235.298540597625\n",
      "95551: 1235.2985405976237\n",
      "77284: 1235.2985405976235\n",
      "768: 1225.5975665113065\n",
      "927: 1117.8383051141816\n",
      "210376: 920.6701252803681\n",
      "95527: 919.6797146521109\n",
      "100130: 916.0190658202705\n",
      "101163: 912.5380530105969\n",
      "95018: 911.1831080078007\n",
      "100646: 909.7095673033034\n",
      "96045: 904.3981315809767\n",
      "66879: 895.7909746044783\n",
      "210305: 893.0386730972414\n",
      "319412: 887.9352083382672\n",
      "571451: 875.7852546255579\n",
      "570985: 871.582558257325\n",
      "544858: 869.6096568148229\n",
      "184142: 863.2307781841786\n",
      "299039: 832.3149809807293\n",
      "49176: 819.8687801616529\n",
      "299040: 784.919578208228\n",
      "319210: 764.4429282969841\n",
      "184332: 748.1100966771163\n",
      "184279: 743.4092370377991\n",
      "743: 694.5573570089932\n",
      "313077: 681.9298001499109\n",
      "331840: 665.4905257656986\n",
      "33: 660.9927237591623\n",
      "184150: 649.4401909507081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Show the first 50 rows with the highest ranks\n",
    "top_50_ranks = ranksRDD.takeOrdered(50, key=lambda x: -x[1])\n",
    "print(\"The first 50 rows with the highest ranks:\")\n",
    "for url, rank in top_50_ranks:\n",
    "    print(\"{}: {}\".format(url, rank))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26f62e5-5a4f-4255-8ea4-5123fa352c97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d370218-c16d-4501-94bf-3e24f560be0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9531b5fb-2dc1-4889-8d3f-9823a9d3a3dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531c6933-2305-40a0-b29f-b59eee95d86b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46223041-053c-437d-943e-d12839569fa1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
